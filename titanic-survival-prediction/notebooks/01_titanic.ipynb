{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e387d57e",
   "metadata": {},
   "source": [
    "# Titanic (Kaggle) ‚Äî **Meu caderno explicad√£o em 1¬™ pessoa** üö¢üß†\n",
    "\n",
    "> **Contexto:** eu j√° mando bem em Python, mas estou aprendendo ML do zero.  \n",
    "> Aqui eu me explico, passo a passo, o que estou fazendo, por que, e como checo se deu certo.  \n",
    "> No final eu gero `submission.csv` e salvo um modelo pronto para reuso.\n",
    "\n",
    "**Fluxo que vou seguir:** preparar pastas ‚Üí carregar dados ‚Üí EDA e *sanity checks* ‚Üí criar *features* ‚Üí pr√©-processar ‚Üí comparar modelos ‚Üí buscar hiperpar√¢metros ‚Üí interpretar ‚Üí avaliar de verdade ‚Üí gerar *submission*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856c523",
   "metadata": {},
   "source": [
    "## 0) Setup de diret√≥rios, seeds e importa√ß√µes\n",
    "Eu quero que tudo seja reprodut√≠vel e organizado em pastas previs√≠veis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo libs padr√£o: manipula√ß√£o de caminhos/arquivos e ci√™ncia de dados\n",
    "from pathlib import Path                   # caminho de arquivo de forma robusta\n",
    "import numpy as np                         # arrays/matem√°tica r√°pida\n",
    "import pandas as pd                        # DataFrames (tabelas)\n",
    "import matplotlib.pyplot as plt            # gr√°ficos (vou usar SEM seaborn aqui)\n",
    "\n",
    "# Defino uma seed global para dar reprodutibilidade\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Defino a raiz do projeto (ajuste se estiver rodando de outro lugar)\n",
    "BASE_DIR = Path(\"/mnt/data/titanic-ml\")\n",
    "\n",
    "# Defino subpastas que vou usar ao longo do fluxo\n",
    "DATA_RAW = BASE_DIR / \"data\" / \"raw\"              # onde tenho train.csv e test.csv\n",
    "DATA_INTERIM = BASE_DIR / \"data\" / \"interim\"      # se eu quiser salvar alguma limpeza parcial\n",
    "DATA_PROCESSED = BASE_DIR / \"data\" / \"processed\"  # dados prontos p/ modelagem (opcional)\n",
    "OUT_FIGS = BASE_DIR / \"outputs\" / \"figures\"       # destino dos gr√°ficos (png)\n",
    "OUT_MODELS = BASE_DIR / \"outputs\" / \"models\"      # destino do modelo treinado (.joblib)\n",
    "SUBMISSION_PATH = BASE_DIR / \"outputs\" / \"submission.csv\"  # onde vai sair meu csv final\n",
    "\n",
    "# Garante que as pastas existem (isso n√£o apaga nada, s√≥ cria se faltar)\n",
    "for p in [DATA_RAW, DATA_INTERIM, DATA_PROCESSED, OUT_FIGS, OUT_MODELS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Estrutura pronta. Coloque train.csv e test.csv em:\", DATA_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed721223",
   "metadata": {},
   "source": [
    "## 1) Ingest√£o dos dados\n",
    "Eu carrego `train.csv` (tem a coluna-alvo `Survived`) e `test.csv` (sem `Survived`).  \n",
    "Regra que vou me lembrar: **`PassengerId` n√£o entra como feature**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dbbbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aponto para os arquivos brutos\n",
    "train_path = DATA_RAW / \"train.csv\"\n",
    "test_path  = DATA_RAW / \"test.csv\"\n",
    "\n",
    "# Checo se os arquivos existem, sen√£o prefiro falhar cedo com uma mensagem clara\n",
    "assert train_path.exists(), f\"Faltou train.csv em {train_path}\"\n",
    "assert test_path.exists(), f\"Faltou test.csv em {test_path}\"\n",
    "\n",
    "# Leitura com pandas\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test  = pd.read_csv(test_path)\n",
    "\n",
    "# Dou uma espiada r√°pida nas primeiras linhas para ter no√ß√£o do schema\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe178f",
   "metadata": {},
   "source": [
    "## 2) *Sanity checks* e EDA b√°sica\n",
    "Aqui eu me certifico de que os tipos, nulos e distribui√ß√µes est√£o dentro do esperado.  \n",
    "O que eu espero ver nos gr√°ficos/tabelas abaixo:\n",
    "- **Propor√ß√£o de `Survived`**: pra sentir se est√° desbalanceado (spoiler: n√£o √© 50/50).\n",
    "- **Nulos por coluna**: `Age` e `Cabin` costumam ter nulos.\n",
    "- **Histogramas de `Age` e `Fare`**: entender caudas/outliers (fare costuma ter cauda longa).\n",
    "- **Contagens por `Pclass`, `Sex`, `Embarked`**: entender a composi√ß√£o do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb81c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info geral para ver tipos e nulos rapidamente\n",
    "print(df_train.info())\n",
    "\n",
    "# Estat√≠sticas por coluna (incluo objetos para contar categorias distintas etc.)\n",
    "display(df_train.describe(include='all').T)\n",
    "\n",
    "# 2.1 Distribui√ß√£o do alvo Survived\n",
    "(ax := df_train['Survived'].value_counts(normalize=True).sort_index().plot.bar())\n",
    "ax.set_title(\"Propor√ß√£o de Survived (0/1)\"); ax.set_xlabel(\"Survived\"); ax.set_ylabel(\"Propor√ß√£o\")\n",
    "plt.tight_layout(); plt.savefig(OUT_FIGS / \"dist_survived.png\"); plt.show()\n",
    "\n",
    "# 2.2 Nulos por coluna (propor√ß√£o)\n",
    "null_rate = df_train.isna().mean().sort_values(ascending=False)\n",
    "(ax := null_rate.plot.bar())\n",
    "ax.set_title(\"Taxa de nulos por coluna (train)\"); ax.set_ylabel(\"Propor√ß√£o de nulos\")\n",
    "plt.tight_layout(); plt.savefig(OUT_FIGS / \"null_rate_train.png\"); plt.show()\n",
    "\n",
    "# 2.3 Histogramas de Age e Fare (caudas/outliers)\n",
    "plt.figure(); df_train['Age'].hist(bins=30)\n",
    "plt.title(\"Histograma de Age\"); plt.xlabel(\"Age\"); plt.ylabel(\"Contagem\")\n",
    "plt.tight_layout(); plt.savefig(OUT_FIGS / \"hist_age.png\"); plt.show()\n",
    "\n",
    "plt.figure(); df_train['Fare'].hist(bins=30)\n",
    "plt.title(\"Histograma de Fare\"); plt.xlabel(\"Fare\"); plt.ylabel(\"Contagem\")\n",
    "plt.tight_layout(); plt.savefig(OUT_FIGS / \"hist_fare.png\"); plt.show()\n",
    "\n",
    "# 2.4 Contagens por Pclass, Sex, Embarked\n",
    "plt.figure(); df_train['Pclass'].value_counts().sort_index().plot.bar()\n",
    "plt.title(\"Contagem por Pclass\"); plt.tight_layout(); plt.savefig(OUT_FIGS / \"count_pclass.png\"); plt.show()\n",
    "\n",
    "plt.figure(); df_train['Sex'].value_counts().plot.bar()\n",
    "plt.title(\"Contagem por Sex\"); plt.tight_layout(); plt.savefig(OUT_FIGS / \"count_sex.png\"); plt.show()\n",
    "\n",
    "plt.figure(); df_train['Embarked'].value_counts().plot.bar()\n",
    "plt.title(\"Contagem por Embarked\"); plt.tight_layout(); plt.savefig(OUT_FIGS / \"count_embarked.png\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c3892",
   "metadata": {},
   "source": [
    "## 3) Split holdout estratificado (20% valida√ß√£o)\n",
    "Eu crio uma valida√ß√£o **estratificada** (mant√©m propor√ß√£o de `Survived`) para ajustar hiperpar√¢metros e medir desempenho.  \n",
    "Detalhe importante: **`PassengerId` e `Survived` n√£o entram como features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ad6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "train_idx, valid_idx = next(sss.split(df_train, df_train['Survived']))\n",
    "\n",
    "train_df = df_train.iloc[train_idx].reset_index(drop=True)\n",
    "valid_df = df_train.iloc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "# Separo X e y (features e alvo). Tiro PassengerId (id) e Survived (o alvo) de X.\n",
    "X_train_raw = train_df.drop(columns=['Survived', 'PassengerId'])\n",
    "y_train     = train_df['Survived']\n",
    "X_valid_raw = valid_df.drop(columns=['Survived', 'PassengerId'])\n",
    "y_valid     = valid_df['Survived']\n",
    "\n",
    "# No teste s√≥ tiro o PassengerId de X\n",
    "X_test_raw  = df_test.drop(columns=['PassengerId'])\n",
    "\n",
    "X_train_raw.shape, X_valid_raw.shape, X_test_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f6069",
   "metadata": {},
   "source": [
    "## 4) Feature engineering (eu crio sinais √∫teis)\n",
    "Eu gero **Title**, **FamilySize**, **IsAlone**, **CabinInitial** (deck), **TicketPrefix** e fa√ßo uma winsoriza√ß√£o leve em `Age` e `Fare`.  \n",
    "Objetivo: dar ao modelo pistas mais diretas sobre perfil social, grupo familiar e cabine/rota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df252665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garanto que consigo importar os m√≥dulos do meu projeto (src/)\n",
    "import sys\n",
    "if str(BASE_DIR / \"src\") not in sys.path:\n",
    "    sys.path.insert(0, str(BASE_DIR / \"src\"))\n",
    "\n",
    "# Importo as fun√ß√µes que eu escrevi no m√≥dulo de features\n",
    "from features.build_features import add_engineered_features, get_feature_lists\n",
    "\n",
    "# Aplico as transforma√ß√µes em cada split (sempre SEM usar nada do conjunto de teste no treinamento)\n",
    "X_train_fe = add_engineered_features(X_train_raw)\n",
    "X_valid_fe = add_engineered_features(X_valid_raw)\n",
    "X_test_fe  = add_engineered_features(X_test_raw)\n",
    "\n",
    "# Descubro quais colunas s√£o num√©ricas e quais s√£o categ√≥ricas (ap√≥s a engenharia)\n",
    "numeric_features, categorical_features = get_feature_lists(X_train_fe)\n",
    "numeric_features, categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62a27e",
   "metadata": {},
   "source": [
    "## 5) Pr√©-processamento via `ColumnTransformer`\n",
    "Eu configuro pipelines para:\n",
    "- **Num√©ricas**: imputar mediana e padronizar (m√©dia 0, desvio 1)  \n",
    "- **Categ√≥ricas**: imputar moda e One-Hot Encoding (com `ignore`)\n",
    "Tudo isso **dentro** de `Pipeline/ColumnTransformer` para n√£o vazar informa√ß√£o do `valid/test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26215c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preprocess import build_preprocessor\n",
    "preprocessor = build_preprocessor(numeric_features, categorical_features, scaler=True)\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f491a",
   "metadata": {},
   "source": [
    "## 6) Modelagem com `Pipeline` e valida√ß√£o cruzada\n",
    "Agora eu conecto `preprocessor` ‚Üí `modelo` e comparo tr√™s modelos: **LogisticRegression**, **RandomForest**, **GradientBoosting**.  \n",
    "Eu avalio com CV estratificada e v√°rias m√©tricas (quero olhar al√©m de accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b50547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modeling import build_pipelines, evaluate_pipelines\n",
    "\n",
    "# Crio um pipeline por modelo (sempre preprocessor antes do estimador)\n",
    "pipes = build_pipelines(preprocessor)\n",
    "\n",
    "# Uso train+valid juntos para a CV (apenas para comparar modelos de forma est√°vel)\n",
    "res_cv = evaluate_pipelines(pipes, pd.concat([X_train_fe, X_valid_fe]), pd.concat([y_train, y_valid]), n_splits=5)\n",
    "res_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11322bb6",
   "metadata": {},
   "source": [
    "## 7) Busca de hiperpar√¢metros (GridSearchCV pequeno e objetivo)\n",
    "Eu rodo *grid search* com espa√ßos pequenos, otimizando **ROC AUC**.  \n",
    "Depois escolho o melhor modelo pelo maior ROC AUC m√©dio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modeling import small_param_grids, run_grid_search\n",
    "\n",
    "grids = small_param_grids()\n",
    "best_models = {}\n",
    "best_scores = {}\n",
    "\n",
    "for name, pipe in pipes.items():\n",
    "    print(f\"\\n>>> Rodando GridSearch para: {name}\")\n",
    "    gs = run_grid_search(pipe, grids[name], X_train_fe, y_train, n_splits=5)\n",
    "    best_models[name] = gs.best_estimator_\n",
    "    best_scores[name] = gs.best_score_\n",
    "    print(\"Melhor ROC AUC:\", gs.best_score_)\n",
    "    print(\"Melhores params:\", gs.best_params_)\n",
    "\n",
    "best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2f74f",
   "metadata": {},
   "source": [
    "### Campe√£o escolhido\n",
    "Aqui eu pego o nome do modelo com maior ROC AUC e guardo o pipeline completo vencedor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8025b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = max(best_scores, key=best_scores.get)\n",
    "best_estimator = best_models[best_name]\n",
    "print(\"Melhor modelo:\", best_name)\n",
    "best_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a05be8",
   "metadata": {},
   "source": [
    "## 8) Interpreta√ß√£o ‚Äî o que pesou nas decis√µes do modelo?\n",
    "O que eu vou olhar: **import√¢ncia de features** (Top 20).  \n",
    "- Se for linear, uso |coeficiente|.  \n",
    "- Se for √°rvore/ensemble, uso `feature_importances_`.  \n",
    "Isso **n√£o √© causalidade**, √© s√≥ sinal do que o modelo mais usou para separar classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modeling import get_feature_names_from_preprocessor\n",
    "import numpy as np\n",
    "\n",
    "# Reajusto o campe√£o em train+valid para obter import√¢ncias mais est√°veis\n",
    "X_full = pd.concat([X_train_fe, X_valid_fe]).reset_index(drop=True)\n",
    "y_full = pd.concat([y_train, y_valid]).reset_index(drop=True)\n",
    "best_estimator.fit(X_full, y_full)\n",
    "\n",
    "# Extraio os nomes de features depois do preprocessor (inclui colunas OHE)\n",
    "feature_names = get_feature_names_from_preprocessor(best_estimator.named_steps['pre'])\n",
    "\n",
    "# Tento coletar import√¢ncias\n",
    "clf = best_estimator.named_steps['clf']\n",
    "if hasattr(clf, \"coef_\"):\n",
    "    importances = np.abs(clf.coef_).ravel()\n",
    "elif hasattr(clf, \"feature_importances_\"):\n",
    "    importances = clf.feature_importances_\n",
    "else:\n",
    "    importances = None\n",
    "\n",
    "if importances is not None:\n",
    "    order = np.argsort(importances)[::-1][:20]  # pego Top 20\n",
    "    plt.figure()\n",
    "    plt.barh([feature_names[i] for i in order][::-1], importances[order][::-1])\n",
    "    plt.title(\"Import√¢ncia de features (Top 20) ‚Äî interpreta√ß√£o r√°pida\")\n",
    "    plt.tight_layout(); plt.savefig(OUT_FIGS / \"feature_importances_top20.png\"); plt.show()\n",
    "else:\n",
    "    print(\"Esse modelo n√£o exp√µe import√¢ncias diretamente. Tudo bem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3811a6",
   "metadata": {},
   "source": [
    "## 9) Avalia√ß√£o de verdade (com valida√ß√£o) ‚Äî o que eu vou ver\n",
    "Eu quero ver a **qualidade do ranking de probabilidades** (ROC/PR),  \n",
    "a **qualidade do classificador** com um **threshold** (matriz de confus√£o, m√©tricas),  \n",
    "e entender a **calibra√ß√£o** (se as probabilidades \"acreditam\" certo).  \n",
    "Tamb√©m vou inspecionar **thresholds** diferentes, **import√¢ncia por permuta√ß√£o**,  \n",
    "**partial dependence** (efeito marginal) e **vi√©s por segmento**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1817dfd",
   "metadata": {},
   "source": [
    "### 9.1 Curvas ROC e Precision-Recall (o que eu espero ver)\n",
    "- ROC AUC perto de 1 √© √≥timo; perto de 0.5 √© aleat√≥rio.  \n",
    "- PR √© √∫til quando a classe positiva √© mais rara (olho o **AP**).  \n",
    "Se meu modelo estiver razo√°vel, eu devo ver curvas acima das diagonais de refer√™ncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, average_precision_score, precision_recall_curve\n",
    "\n",
    "# Fun√ß√£o utilit√°ria que sempre me devolve \"prob de classe positiva\"\n",
    "def _proba(est, X):\n",
    "    if hasattr(est.named_steps['clf'], \"predict_proba\"):\n",
    "        return est.predict_proba(X)[:, 1]\n",
    "    elif hasattr(est.named_steps['clf'], \"decision_function\"):\n",
    "        # normalizo a decision_function para 0..1 s√≥ pra comparar\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        s = est.decision_function(X).reshape(-1,1)\n",
    "        return MinMaxScaler().fit_transform(s).ravel()\n",
    "    else:\n",
    "        return est.predict(X).astype(float)\n",
    "\n",
    "y_valid_proba = _proba(best_estimator, X_valid_fe)\n",
    "\n",
    "# ROC\n",
    "fpr, tpr, _ = roc_curve(y_valid, y_valid_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],'--', label=\"Aleat√≥rio\")\n",
    "plt.xlabel(\"Falso positivo (FPR)\"); plt.ylabel(\"Verdadeiro positivo (TPR)\")\n",
    "plt.title(\"Curva ROC (valida√ß√£o)\"); plt.legend(); plt.tight_layout()\n",
    "plt.savefig(OUT_FIGS / \"roc_curve_valid.png\"); plt.show()\n",
    "\n",
    "# PR\n",
    "prec, rec, _ = precision_recall_curve(y_valid, y_valid_proba)\n",
    "ap = average_precision_score(y_valid, y_valid_proba)\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, label=f\"AP = {ap:.3f}\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Curva Precision-Recall (valida√ß√£o)\"); plt.legend(); plt.tight_layout()\n",
    "plt.savefig(OUT_FIGS / \"pr_curve_valid.png\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7466c",
   "metadata": {},
   "source": [
    "### 9.2 Matriz de confus√£o + relat√≥rio (threshold 0.5)\n",
    "Aqui eu vejo a troca entre **acertar positivos** (TP) e **evitar falsos positivos** (FP).  \n",
    "Se eu precisar priorizar **recall** (n√£o perder sobreviventes), posso depois mexer no threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac90c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "y_valid_pred = (y_valid_proba >= 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_valid, y_valid_pred)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot(values_format=\"d\")\n",
    "plt.title(\"Matriz de confus√£o (thr=0.5)\"); plt.tight_layout()\n",
    "plt.savefig(OUT_FIGS / \"confusion_matrix_thr05.png\"); plt.show()\n",
    "\n",
    "print(classification_report(y_valid, y_valid_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ffcf6",
   "metadata": {},
   "source": [
    "### 9.3 Explorando thresholds (o que acontece se eu mexer no 0.5?)\n",
    "Eu vario o limiar de decis√£o e vejo **F1**, **precision**, **recall**, **accuracy**.  \n",
    "Se eu quisesse \"salvar mais gente\", eu subiria o **recall** (normalmente ajustando o threshold para baixo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e3133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "thr_list = np.linspace(0.05, 0.95, 19)\n",
    "rows = []\n",
    "for thr in thr_list:\n",
    "    yhat = (y_valid_proba >= thr).astype(int)\n",
    "    rows.append({\n",
    "        \"thr\": float(thr),\n",
    "        \"f1\": f1_score(y_valid, yhat),\n",
    "        \"precision\": precision_score(y_valid, yhat),\n",
    "        \"recall\": recall_score(y_valid, yhat),\n",
    "        \"accuracy\": accuracy_score(y_valid, yhat)\n",
    "    })\n",
    "df_thr = pd.DataFrame(rows)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_thr[\"thr\"], df_thr[\"f1\"], label=\"F1\")\n",
    "plt.plot(df_thr[\"thr\"], df_thr[\"precision\"], label=\"Precision\")\n",
    "plt.plot(df_thr[\"thr\"], df_thr[\"recall\"], label=\"Recall\")\n",
    "plt.plot(df_thr[\"thr\"], df_thr[\"accuracy\"], label=\"Accuracy\")\n",
    "plt.xlabel(\"Threshold\"); plt.ylabel(\"Score\"); plt.title(\"M√©tricas vs Threshold (valida√ß√£o)\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(OUT_FIGS / \"metrics_vs_threshold.png\"); plt.show()\n",
    "\n",
    "best_thr = float(df_thr.iloc[df_thr[\"f1\"].idxmax()][\"thr\"])\n",
    "print(f\"Threshold que maximizou F1: {best_thr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651c95f",
   "metadata": {},
   "source": [
    "### 9.4 Calibra√ß√£o (as probabilidades fazem sentido?)\n",
    "Eu comparo probabilidade prevista √ó fra√ß√£o real de positivos.  \n",
    "- Se a curva segue a diagonal, as probabilidades est√£o **bem calibradas**.\n",
    "- O **Brier score** mede o erro quadr√°tico das probabilidades (quanto menor, melhor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_valid, y_valid_proba, n_bins=10, strategy=\"uniform\")\n",
    "brier = brier_score_loss(y_valid, y_valid_proba)\n",
    "plt.figure()\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Modelo\")\n",
    "plt.plot([0,1],[0,1],\"--\", label=\"Perfeito\")\n",
    "plt.xlabel(\"Prob. prevista\"); plt.ylabel(\"Fra√ß√£o de positivos\")\n",
    "plt.title(f\"Curva de calibra√ß√£o (Brier={brier:.3f})\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(OUT_FIGS / \"calibration_curve.png\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf716f",
   "metadata": {},
   "source": [
    "### 9.5 Import√¢ncia por permuta√ß√£o (robusta e compar√°vel)\n",
    "Eu embaralho uma coluna por vez e vejo quanto o desempenho piora.  \n",
    "Se piora muito, aquela feature era importante para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from models.modeling import get_feature_names_from_preprocessor\n",
    "import numpy as np\n",
    "\n",
    "r = permutation_importance(best_estimator, X_valid_fe, y_valid, n_repeats=10,\n",
    "                           random_state=RANDOM_STATE, n_jobs=-1, scoring=\"f1\")\n",
    "fn = np.array(get_feature_names_from_preprocessor(best_estimator.named_steps[\"pre\"]))\n",
    "idx = np.argsort(r.importances_mean)[-20:]  # Top 20\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(fn[idx], r.importances_mean[idx])\n",
    "plt.title(\"Import√¢ncia por permuta√ß√£o (Top 20)\")\n",
    "plt.tight_layout(); plt.savefig(OUT_FIGS / \"perm_importances_top20.png\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48a2e0",
   "metadata": {},
   "source": [
    "### 9.6 Partial Dependence (efeito marginal de Age e Fare)\n",
    "Eu olho o efeito m√©dio de variar **Age** e **Fare**, mantendo o restante nos dados reais.  \n",
    "Se a curva sobe com `Fare`, por exemplo, indica que tarifas mais altas tendem a aumentar probabilidade de sobreviv√™ncia (coerente com Pclass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d48ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "plt.figure()\n",
    "PartialDependenceDisplay.from_estimator(best_estimator, X_full, features=[\"Age\", \"Fare\"])\n",
    "plt.suptitle(\"Partial Dependence: Age & Fare\", y=1.02)\n",
    "plt.tight_layout(); plt.savefig(OUT_FIGS / \"pdp_age_fare.png\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8659a",
   "metadata": {},
   "source": [
    "### 9.7 Vi√©s/segmento (Sex, Pclass)\n",
    "Eu quebro as m√©tricas por grupos (ex.: **Sex** e **Pclass**).  \n",
    "Se tiver disparidades claras, eu anoto e penso em mitiga√ß√µes (coletas, features, thresholds por grupo etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46eb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "valid_df = valid_df.copy()\n",
    "valid_df[\"y_true\"] = y_valid.values\n",
    "valid_df[\"y_proba\"] = y_valid_proba\n",
    "valid_df[\"y_pred\"] = (y_valid_proba >= 0.5).astype(int)\n",
    "\n",
    "def segment_report(df, col):\n",
    "    out = []\n",
    "    for g, gdf in df.groupby(col):\n",
    "        # Protejo as m√©tricas caso algum grupo tenha s√≥ uma classe\n",
    "        try: prec = precision_score(gdf[\"y_true\"], gdf[\"y_pred\"])\n",
    "        except: prec = float(\"nan\")\n",
    "        try: rec = recall_score(gdf[\"y_true\"], gdf[\"y_pred\"])\n",
    "        except: rec = float(\"nan\")\n",
    "        acc = (gdf[\"y_true\"]==gdf[\"y_pred\"]).mean()\n",
    "        out.append({\"grupo\": g, \"n\": len(gdf), \"acc\": acc, \"precision\": prec, \"recall\": rec,\n",
    "                    \"pos_rate_pred\": gdf[\"y_pred\"].mean(), \"proba_media\": gdf[\"y_proba\"].mean()})\n",
    "    return pd.DataFrame(out).sort_values(\"n\", ascending=False)\n",
    "\n",
    "print(\"Por Sex\"); display(segment_report(valid_df, \"Sex\"))\n",
    "print(\"Por Pclass\"); display(segment_report(valid_df, \"Pclass\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60782f1",
   "metadata": {},
   "source": [
    "### 9.8 Casos incertos (pr√≥ximos de 0.5)\n",
    "Eu quero olhar manualmente os casos que meu modelo achou **mais duvidosos** (probabilidade ~ 0.5).  \n",
    "Isso me ajuda a entender onde coletar dados melhores ou criar novas features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_review = valid_df.copy()\n",
    "to_review[\"dist_0p5\"] = (to_review[\"y_proba\"] - 0.5).abs()\n",
    "cols_show = [\"PassengerId\",\"Sex\",\"Pclass\",\"Age\",\"Fare\",\"y_true\",\"y_proba\",\"dist_0p5\"]\n",
    "display(to_review.sort_values(\"dist_0p5\").head(20)[cols_show])\n",
    "\n",
    "# Se eu quiser salvar os 100 mais incertos:\n",
    "to_review.sort_values(\"dist_0p5\").head(100)[cols_show].to_csv(OUT_FIGS.parent / \"cases_to_review.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbdf8c",
   "metadata": {},
   "source": [
    "### 9.9 Learning curve (vale a pena ter mais dados?)\n",
    "Eu vejo como a m√©trica evolui com mais exemplos de treino.  \n",
    "Se a curva de valida√ß√£o ainda sobe bastante, mais dados podem ajudar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "\n",
    "sizes, train_scores, val_scores = learning_curve(best_estimator, X_full, y_full,\n",
    "                                                 cv=5, scoring=\"f1\",\n",
    "                                                 train_sizes=np.linspace(0.1, 1.0, 6),\n",
    "                                                 n_jobs=-1, random_state=RANDOM_STATE)\n",
    "plt.figure()\n",
    "plt.plot(sizes, train_scores.mean(axis=1), marker=\"o\", label=\"Treino\")\n",
    "plt.plot(sizes, val_scores.mean(axis=1), marker=\"o\", label=\"Valida√ß√£o\")\n",
    "plt.xlabel(\"Tamanho do treino\"); plt.ylabel(\"F1\"); plt.title(\"Learning Curve\")\n",
    "plt.legend(); plt.tight_layout(); plt.savefig(OUT_FIGS / \"learning_curve.png\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3ffb5",
   "metadata": {},
   "source": [
    "## 10) Visualiza√ß√µes de neg√≥cio (o que eu espero encontrar)\n",
    "Agora, al√©m do modelo, eu olho padr√µes do problema em si.  \n",
    "- **Sex √ó Pclass**: grupos com maior taxa de sobreviv√™ncia (espera-se mulheres e classes mais altas).\n",
    "- **FamilySize/IsAlone**: fam√≠lias m√©dias podem ter vantagem; sozinho tende a ser pior.\n",
    "- **CabinInitial** (deck): pode capturar localiza√ß√£o no navio; *NA* √© muita gente sem cabine registrada.\n",
    "- **TicketPrefix**: as companhias/rotas podem ter padr√µes (cuidado: amostras pequenas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533927cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 10.1 Sex √ó Pclass\n",
    "gp = (df_train.groupby([\"Sex\",\"Pclass\"])[\"Survived\"]\n",
    "      .agg([\"mean\",\"count\"])\n",
    "      .reset_index()\n",
    "      .sort_values([\"Sex\",\"Pclass\"]))\n",
    "\n",
    "labels = [f\"{row.Sex}-P{int(row.Pclass)}\" for _, row in gp.iterrows()]\n",
    "rates  = gp[\"mean\"].to_list()\n",
    "counts = gp[\"count\"].to_list()\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(x, rates)\n",
    "for i, (r, n) in enumerate(zip(rates, counts)):\n",
    "    plt.text(i, r + 0.01, f\"n={n}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "plt.ylim(0,1.05); plt.xticks(x, labels, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Taxa de sobreviv√™ncia\"); plt.xlabel(\"Grupo\")\n",
    "plt.title(\"Sobreviv√™ncia por grupo (Sex √ó Pclass)\")\n",
    "plt.tight_layout(); plt.savefig(OUT_FIGS / \"survival_by_sex_pclass.png\"); plt.show()\n",
    "\n",
    "# 10.2 FamilySize / IsAlone\n",
    "tmp = df_train.copy()\n",
    "tmp[\"FamilySize\"] = tmp[\"SibSp\"].fillna(0) + tmp[\"Parch\"].fillna(0) + 1\n",
    "tmp[\"IsAlone\"] = (tmp[\"FamilySize\"] == 1).astype(int)\n",
    "\n",
    "def bucket_family_size(x):\n",
    "    if x <= 1: return \"1\"\n",
    "    if x == 2: return \"2\"\n",
    "    if 3 <= x <= 4: return \"3-4\"\n",
    "    if 5 <= x <= 6: return \"5-6\"\n",
    "    return \"7+\"\n",
    "\n",
    "tmp[\"FamilyBucket\"] = tmp[\"FamilySize\"].apply(bucket_family_size)\n",
    "\n",
    "gp1 = (tmp.groupby(\"FamilyBucket\")[\"Survived\"]\n",
    "         .agg([\"mean\",\"count\"])\n",
    "         .reindex([\"1\",\"2\",\"3-4\",\"5-6\",\"7+\"]))\n",
    "\n",
    "labels1 = gp1.index.tolist(); rates1 = gp1[\"mean\"].to_list(); counts1 = gp1[\"count\"].to_list()\n",
    "plt.figure(figsize=(12,5))\n",
    "# (a) FamilyBucket\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "x1 = np.arange(len(labels1)); ax1.bar(x1, rates1)\n",
    "for i,(r,n) in enumerate(zip(rates1, counts1)):\n",
    "    ax1.text(i, r+0.01, f\"n={n}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "ax1.set_ylim(0,1.05); ax1.set_xticks(x1, labels1); ax1.set_title(\"Por FamilySize (bucket)\"); ax1.set_ylabel(\"Taxa\")\n",
    "# (b) IsAlone\n",
    "gp2 = tmp.groupby(\"IsAlone\")[\"Survived\"].agg([\"mean\",\"count\"]).reset_index().sort_values(\"IsAlone\")\n",
    "labels2 = [\"Acompanhado (0)\",\"Sozinho (1)\"]; rates2 = gp2[\"mean\"].to_list(); counts2 = gp2[\"count\"].to_list()\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "x2 = np.arange(len(labels2)); ax2.bar(x2, rates2)\n",
    "for i,(r,n) in enumerate(zip(rates2, counts2)):\n",
    "    ax2.text(i, r+0.01, f\"n={n}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "ax2.set_ylim(0,1.05); ax2.set_xticks(x2, labels2, rotation=15); ax2.set_title(\"Por IsAlone\")\n",
    "plt.suptitle(\"Efeito de fam√≠lia\"); plt.tight_layout(); plt.savefig(OUT_FIGS / \"family_effect.png\"); plt.show()\n",
    "\n",
    "# 10.3 CabinInitial\n",
    "tmp = df_train.copy()\n",
    "tmp[\"CabinInitial\"] = tmp[\"Cabin\"].astype(str).str[0]\n",
    "tmp.loc[tmp[\"Cabin\"].isna() | (tmp[\"Cabin\"]==\"\"), \"CabinInitial\"] = \"NA\"\n",
    "\n",
    "gp = (tmp.groupby(\"CabinInitial\")[\"Survived\"]\n",
    "        .agg([\"mean\",\"count\"])\n",
    "        .sort_values(\"count\", ascending=False)).head(10)\n",
    "\n",
    "labels = gp.index.tolist(); rates = gp[\"mean\"].to_list(); counts = gp[\"count\"].to_list()\n",
    "x = np.arange(len(labels))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(x, rates)\n",
    "for i,(r,n) in enumerate(zip(rates, counts)):\n",
    "    plt.text(i, r+0.01, f\"n={n}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "plt.ylim(0,1.05); plt.xticks(x, labels); plt.xlabel(\"Deck (letra) / NA\"); plt.ylabel(\"Taxa de sobreviv√™ncia\")\n",
    "plt.title(\"Sobreviv√™ncia por CabinInitial (Top 10 em n)\")\n",
    "plt.tight_layout(); plt.savefig(OUT_FIGS / \"survival_by_cabininitial_top10.png\"); plt.show()\n",
    "\n",
    "# 10.4 TicketPrefix\n",
    "def ticket_prefix(s):\n",
    "    if not isinstance(s, str): return \"None\"\n",
    "    s2 = re.sub(r\"[./]\", \"\", s).strip()\n",
    "    m = re.match(r\"([A-Za-z]+)\", s2)\n",
    "    return m.group(1) if m else \"None\"\n",
    "\n",
    "tmp = df_train.copy()\n",
    "tmp[\"TicketPrefix\"] = tmp[\"Ticket\"].apply(ticket_prefix)\n",
    "\n",
    "gp = (tmp.groupby(\"TicketPrefix\")[\"Survived\"]\n",
    "        .agg([\"mean\",\"count\"])\n",
    "        .sort_values(\"count\", ascending=False)).head(10)\n",
    "\n",
    "labels = gp.index.tolist(); rates = gp[\"mean\"].to_list(); counts = gp[\"count\"].to_list()\n",
    "x = np.arange(len(labels))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(x, rates)\n",
    "for i,(r,n) in enumerate(zip(rates, counts)):\n",
    "    plt.text(i, r+0.01, f\"n={n}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "plt.ylim(0,1.05); plt.xticks(x, labels, rotation=45, ha=\"right\"); plt.xlabel(\"Prefixo\"); plt.ylabel(\"Taxa de sobreviv√™ncia\")\n",
    "plt.title(\"Sobreviv√™ncia por TicketPrefix (Top 10 em n)\")\n",
    "plt.tight_layout(); plt.savefig(OUT_FIGS / \"survival_by_ticketprefix_top10.png\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5d7e3",
   "metadata": {},
   "source": [
    "## 11) Gera√ß√£o do `submission.csv`\n",
    "Eu j√° tenho o **melhor pipeline ajustado** (no passo 8).  \n",
    "Agora eu gero as probabilidades no `test`, aplico o threshold 0.5 e salvo `PassengerId,Survived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pego probabilidade de classe positiva do conjunto de teste\n",
    "if hasattr(best_estimator.named_steps['clf'], 'predict_proba'):\n",
    "    test_proba = best_estimator.predict_proba(X_test_fe)[:, 1]\n",
    "elif hasattr(best_estimator.named_steps['clf'], 'decision_function'):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    s = best_estimator.decision_function(X_test_fe).reshape(-1,1)\n",
    "    test_proba = MinMaxScaler().fit_transform(s).ravel()\n",
    "else:\n",
    "    test_proba = best_estimator.predict(X_test_fe).astype(float)\n",
    "\n",
    "# Aplico threshold 0.5 (poderia ajustar com base no que vi em 9.3)\n",
    "test_pred = (test_proba >= 0.5).astype(int)\n",
    "\n",
    "# Monto o DataFrame no formato que o Kaggle pede\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": df_test[\"PassengerId\"],\n",
    "    \"Survived\": test_pred\n",
    "})\n",
    "submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(\"Submission salva em:\", SUBMISSION_PATH)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd99177",
   "metadata": {},
   "source": [
    "## 12) Salvar o melhor modelo (reuso futuramente)\n",
    "Eu salvo o pipeline completo (pr√©-processamento + modelo). Assim, ao carregar, √© s√≥ dar `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b263d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "model_path = OUT_MODELS / \"model.joblib\"\n",
    "dump(best_estimator, model_path)\n",
    "print(\"Modelo salvo em:\", model_path)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
